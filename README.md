# Задание 1

Опишите основные преимущества использования масштабирования методами:

активный master-сервер и пассивный репликационный slave-сервер;
master-сервер и несколько slave-серверов;
Дайте ответ в свободной форме.

Ответ:

1. Активный master и пассивный slave (один репликационный сервер):
Такая схема в первую очередь даёт резервирование и повышение отказоустойчивости. Master обрабатывает все записываемые (WRITE) операции, а slave просто копирует данные в фоне. Если вдруг master выйдет из строя, можно вручную (или автоматически, если настроить) переключиться на slave — он уже содержит почти актуальные данные. Плюс, такой slave иногда можно использовать для резервного копирования, чтобы не нагружать основной сервер. Однако масштабировать чтение (READ) особо не получится — только один slave, и он обычно не используется под нагрузку.

2. Master и несколько slave-серверов:
Здесь уже появляется горизонтальное масштабирование операций чтения. Поскольку большинство приложений читают данные гораздо чаще, чем пишут, можно распределить запросы SELECT между несколькими slave-серверами. Это снижает нагрузку на master и улучшает общую производительность. Также сохраняются преимущества резервирования — если один slave упадёт, остальные продолжат работать. Правда, усложняется архитектура: нужно следить за репликацией на всех узлах, возможны небольшие задержки (lag), и синхронизация требует больше ресурсов на стороне master (он отправляет бинарные логи всем slave).

Первый вариант — про надёжность и простоту, второй — про производительность и масштабируемость, но с чуть большей сложностью.

# Задание 2

Разработайте план для выполнения горизонтального и вертикального шаринга базы данных. База данных состоит из трёх таблиц:

пользователи,
книги,
магазины (столбцы произвольно).
Опишите принципы построения системы и их разграничение или разбивку между базами данных.

Пришлите блоксхему, где и что будет располагаться. Опишите, в каких режимах будут работать сервера.


Ответ:
Общее понимание
Вертикальный шардинг — это разделение таблиц по столбцам или по функциональности: разные таблицы (или части одной таблицы) хранятся в разных базах.
Горизонтальный шардинг — это разделение строк одной и той же таблицы по какому-то ключу (например, по региону, ID пользователя и т.д.).

1. План вертикального шардинга

Принцип:
Разделяем таблицы по смыслу и нагрузке между разными серверами.

Разбивка:

Сервер A (Auth & Users) — хранит таблицу пользователи
(часто используется для авторизации, логинов, профилей)

Сервер B (Catalog) — хранит таблицу книги
(каталог товаров, часто читается, редко меняется)

Сервер C (Retail) — хранит таблицу магазины
(информация о точках продаж, адреса, часы работы и т.п.)

Режимы работы:
Все серверы работают в режиме master (запись + чтение).
Если нужно — можно добавить репликацию (slave) к каждому для отказоустойчивости, но это уже не шардинг, а репликация.

Приложение само решает, к какому серверу обращаться в зависимости от операции:
Авторизация → Сервер A
Поиск книги → Сервер B
Найти ближайший магазин → Сервер C

Плюсы: простота, логическое разделение, снижение нагрузки на один сервер.
Минусы: нельзя легко делать JOIN между таблицами (например, "пользователи + их заказы в магазинах"), нужно делать несколько запросов.

###
###                     ┌──────────────────────┐
###                     │   Приложение (App)   │
###                     └──────────┬───────────┘
###                                │
###        ┌───────────────────────┼───────────────────────┐
###        │                       │                       │
###┌───────▼───────┐     ┌─────────▼─────────┐   ┌─────────▼─────────┐
###│  База: Users  │     │   База: Books     │   │  База: Shops      │
###│  Таблица:     │     │  Таблица:         │   │  Таблица:         │
###│  -пользователи│     │  - книги          │   │  - магазины       │
###│               │     │                   │   │                   │
###│ Режим: master │     │ Режим: master     │   │ Режим: master     │
###└───────────────┘     └───────────────────┘   └───────────────────┘
###

2. План горизонтального шардинга

Принцип:
Разбиваем одну большую таблицу на части по строкам, используя шард-ключ.

Допустим, самая большая и нагруженная таблица — пользователи (миллионы записей). Таблицы книги и магазины относительно небольшие — их можно оставить целиком на одном сервере или тоже шардировать, но для примера сосредоточимся на пользователях.

Шард-ключ:
Выбираем user_id или регион пользователя (например, страна или город).

Разбивка:
Шард 1 (Users Shard 1) — пользователи с user_id % 2 = 0
Шард 2 (Users Shard 2) — пользователи с user_id % 2 = 1
Таблицы книги и магазины хранятся полностью на отдельном сервере каталога (или реплицируются на все шарды, если нужен локальный доступ).

Архитектура:
Сервер U1 — только часть пользователей (шард 1)
Сервер U2 — только часть пользователей (шард 2)
Сервер Catalog — полные таблицы книги и магазины (режим master, возможно с репликами)

Режимы работы:
Серверы U1 и U2 — master для своей части данных.
Catalog — master (или master + slaves для масштабирования чтения).
Приложение использует роутер шардов: по user_id определяет, к какому серверу (U1 или U2) идти.

Плюсы: отлично масштабирует большие таблицы, распределяет нагрузку.
Минусы: сложнее делать глобальные запросы ("все пользователи"), нельзя легко менять шард-ключ, JOIN с другими таблицами требует осторожности.

###                     ┌──────────────────────┐
###                     │   Приложение (App)   │
###                     └──────────┬───────────┘
###                                │
###        ┌───────────────────────┼───────────────────────┐
###        │                       │                       │
###┌───────▼───────┐     ┌─────────▼─────────┐   ┌─────────▼─────────┐
###│ Users Shard 1 │     │  Users Shard 2    │   │   Catalog Server  │
###│(user_id % 2=0)│     │ (user_id % 2=1)   │   │  - книги          │
###│ Режим: master │     │ Режим: master     │   │  - магазины       │
###└───────────────┘     └───────────────────┘   │  Режим: master    │
###                                              └───────────────────┘

